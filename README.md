# ğŸŒŒ EchoLLM

> **EchoLLM** â€” a lightweight, flexible, and **beautifully simple** framework for Large Language Model (LLM)
> applications.  
> It combines **smart similarity caching**, a **plug-and-play architecture**, and a **super easy API** to make
> LLM-powered systems practical, efficient, and fun to build.

---

## ğŸš€ Motivation

Why another LLM framework?  
Because most frameworks either feel **bloated and rigid** or **too barebones to be useful**.  
With EchoLLM, our key contributions are:

1. **Smart Similarity Cache** â€” repeated or similar prompts donâ€™t need to hit the model again. We integrate
   similarityâ€“based caching **directly into the LLM call layer**, saving money, latency, and making caching
   *first-class* instead of an afterthought.
2. **Minimal, Simple Framework** â€” one of the easiest APIs for LLMs youâ€™ll find. Import, call, get results. Done.
3. **Extreme Flexibility** â€” every component (cache algorithm, LLM backend, distance metric, database, etc.) is *
   *replaceable and swappable**. Want to test a custom cache policy? Just drop it in. Want to try a different LLM
   provider or similarity function? Plug it in with no friction.
4. **Super Easy API** â€” clear, minimal, Pythonic. No boilerplate, no YAML jungles, no â€œhidden magic.â€ You stay in
   control.

---

## âœ¨ Features

- ğŸ”„ **Smart similarity cache** with pluggable policies.
- ğŸ§© **Composable framework** â€” swap LLMs, databases, distance metrics, or cache strategies effortlessly.
- ğŸª¶ **Tiny API surface** â€” learn it in minutes.
- âš¡ **Fast prototyping** â€” go from idea â†’ working code with almost no setup.
- ğŸ“¦ **Production-ready** â€” caching, similarity search, modularity baked in.
- ğŸ“š **Extensible design** â€” each layer is cleanly separated.

---

## ğŸ“ Architecture

EchoLLM is built around **clean modular components**:

![System UML Diagram](EchoLLM-Design.png)

- **EchoLLM**: the framework's entrypoint.
- **LLM Backend**: pluggable provider (OpenAI, local models, etc.).
- **Cache Manager**: integrates *smart similarity* for reusing close-enough results.
- **Storage Client**: handles persistence (databases, file system, or pluggable storage backends).
- **Text Similarity**: pluggable text (prompt) embedders and vector distance metrics for calculating similarities
  between requests.

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/BlankShahar/EchoLLM.git
cd EchoLLM
python -m venv .venv && source .venv/bin/activate   # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
```

---

## âš¡ Toy Code Example


### Smart Similarity Cache

```python
from echollm import EchoLLM

echo_llm = EchoLLM(cache=SimilarityCache(...), llm=...)

# First call: misses the cache, hits the LLM
res1 = echo_llm.ask("What is an echo?")
print(res1)

# Second call: retrieved instantly from similarity cache (no LLM's API cost)
res2 = echo_llm.ask("Whatâ€™s an echo?")
print(res2)
```
Check out the full example usage in [example_usage.py](./_example.py) module.
Here's an output example:
```shell
>>> ask('Write me a short script of calculator in python')
INFO:EchoLLM:Cache Miss
INFO:EchoLLM:LLM response took 10156.13ms
INFO:httpx:HTTP Request: POST http://localhost:11434/api/pull "HTTP/1.1 200 OK"
python
def calculator():
  """A simple calculator in Python."""
...
-------------
>>> ask('Make a simple calculator in python')
INFO:EchoLLM:Cache Hit
python
def calculator():
  """A simple calculator in Python."""
...
-------------
>>> ask('Hi')
INFO:EchoLLM:Cache Miss
INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
INFO:EchoLLM:LLM response took 8586.60ms
Hey there! Howâ€™s your day going so far? ğŸ˜Š 
...
```
---

## ğŸ“‚ Project Structure

```
EchoLLM/
â”œâ”€ cache/                 # Cache algorithm implemenations
â”œâ”€ llm/                   # LLM backends
â”œâ”€ text_similarity/       # Embeddings + similarity (vector distance) logic
â”œâ”€ echo_llm.py            # Main EchoLLM API
â”œâ”€ _example.py            # Demo script with toy examples and usage showcase
â””â”€ requirements.txt       # Dependencies
```

---


## ğŸ›£ï¸ Roadmap

- âœ… Core framework & cache
- ğŸš§ More sophisticated cache algorithms
- ğŸš§ More LLM backends (Anthropic, local models, HuggingFace)
- ğŸš§ Optional Redis/Postgres cache storage implementations
- ğŸš§ More similarity metrics out-of-the-box
- ğŸš§ Tracing & observability hooks

---

### ğŸ§  Supported Cache Policies

| Policy | Notes |
|-------|-------|
| ğŸ”¹ **LRU** | Standard least-recently-used eviction |
| ğŸ”¸ **LFU** | Tracks usage frequency to guide eviction |
| ğŸ”¹ **FIFO** | First-in-first-out queue behavior |
| ğŸ”¸ **RR** | Random replacement eviction |
| ğŸš€ **Adaptive Pipeline Cache** | [External implementation](https://github.com/NadavKeren/python-adaptive-pipeline-cache) â€“ adaptive & workload-aware |

## ğŸ¤ Contributing

PRs welcome!

- Keep things minimal and pluggable.
- Add docstrings & tests.
- Run `_example.py` before submitting.

---

## ğŸ“œ License

TBD.

---

## ğŸ’¡ Inspiration

EchoLLM was inspired by prior work on caching for LLMs, such as [GPTCache](https://github.com/zilliztech/GPTCache), which demonstrated the value of avoiding redundant API calls by reusing previous responses.  

Our name **EchoLLM** comes from this very idea:  
we integrate a **caching framework as the frontend**, placed before the LLM backend.  
Whenever a cache hit occurs (e.g., for similar prompts), the cache simply **â€œechoesâ€ the previous LLM response**, saving both cost and latency.  

In this way, EchoLLM emphasizes that the cache is not just a side optimization, but a **first-class design principle** in the system.
